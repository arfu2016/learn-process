{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data.csv')\n",
    "X = dataset.iloc[ : , :-1].values\n",
    "Y = dataset.iloc[ : , 3].values\n",
    "\n",
    "#  -----------------------------------------------------------\n",
    "\n",
    "xl = pd.ExcelFile(\"dognition_data_aggregated_by_dogid.xlsx\")\n",
    "# pd.read_excel()\n",
    "# xlrd is used by pandas\n",
    "xl.sheet_names\n",
    "print 'Sheet name of the excel file:', xl.sheet_names[0]\n",
    "df = xl.parse(\"dog_id_max_ranks\")\n",
    "# produce a dataframe\n",
    "# df.head()\n",
    "\n",
    "\n",
    "# For each column, show the number of nan values\n",
    "\n",
    "# Search for the nan values in each column\n",
    "print df.isnull().sum()\n",
    "# df.isnull() is also a dataframe, and then sum() by columns\n",
    "\n",
    "\n",
    "# Search for the nan values in each row\n",
    "\n",
    "num = np.sum(df.isnull().values, axis = 1).tolist()\n",
    "l1 = list(set(num))\n",
    "l1.sort()\n",
    "fre = {i:num.count(i) for i in l1}\n",
    "dt = pd.DataFrame(fre.items(), columns=['Number of NaNs', 'Count'])\n",
    "print dt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Handling the missing data and outliers\n",
    "\n",
    "如果标出的是None，可以不去处理；也可以把整个样本除去；也可以推测一个合理的值，但是，对于时间序列，不能用未来倒推过去\n",
    "\n",
    "更难处理的missing data是，标出来是有数据的，但出错了， 也就是说，是outlier，需要检测出来，检测出来以后，标成missing data\n",
    "\n",
    "outlier是假数据、出错的数据，但本质上是没法用当前模型理解、预测的数据。比如预测股价，股价当然是真实的，但如果没法用我们的模型去理解和预测，也要当成outlier，不懂不碰，不纳入我们需要理解的数据，否则反而会对模型造成干扰。比如用统计模型取理解股价，统计模型要求数据的可重复性要高，得是大样本数据才行，那么，对于极端数据，在统计模型看来就是outlier（因为过不了统计检验，对统计模型是没用的），但是，如果我们有一种很强的机制模型可以解释预测这些outlier的话，这些点就应该纳入模型，不再当做outlier。\n",
    "\n",
    "qq plot是用来找outlier的一种办法，比如理论上是normal distribution，但用qq plot看出分布肥尾的话，就要考虑肥尾是不是outlier。\n",
    "\n",
    "We can also use box plot to detect outliers for continuous variables. Q1 is the first quartile and Q3 is the third quartile. Any value, which is beyond the range of Q1-1.5 x IQR to Q3+1.5 x IQR, will be regarded an outlier. IQR is interquartile range. 这样做的前提是，我们的模型要求数据是正态分布，我们假设，数据点的分布符合正态分布，这个前提下，the range of Q1-1.5 x IQR to Q3+1.5 x IQR之外的概率只有1-50%-2*24.65% = 0.6%. 如果数据量下的话，不应该出现，所以是错误数据，是outlier.\n",
    "\n",
    "Another criteria is: data points, three or more standard deviation away from mean are considered outlier. Outlier detection is merely a special case of the examination of data for influential data points and it also depends on the business understanding.\n",
    "\n",
    "PCA can also be used in outlier detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values = \"NaN\", strategy = \"mean\", axis = 0)\n",
    "imputer = imputer.fit(X[ : , 1:3])\n",
    "X[ : , 1:3] = imputer.transform(X[ : , 1:3])\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "db = df.drop(df.index[[num.index(22)]])\n",
    "# Simply handling the missing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: drop unnecessary columns and convert type for some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 1: drop unnecessary columns\n",
    "fields_to_drop = ['instant', 'dteday', 'atemp', 'workingday',\n",
    "                  'casual', 'registered']\n",
    "cnt_df = cnt_df.drop(fields_to_drop, axis=1)\n",
    "# step 2: type converting\n",
    "fields_to_convert = ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday',\n",
    "                     'weathersit']\n",
    "for field in fields_to_convert:\n",
    "    # cnt_df[field] = cnt_df[field].astype('object')\n",
    "    cnt_df[field] = cnt_df[field].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Encoding categorical data if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[ : , 0] = labelencoder_X.fit_transform(X[ : , 0])\n",
    "\n",
    "# Creating a dummy variable\n",
    "# a dummy variable is a categorical variable using 0 and 1\n",
    "\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "labelencoder_Y = LabelEncoder()\n",
    "Y =  labelencoder_Y.fit_transform(Y)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "db2 = db.copy()\n",
    "for y in db2.columns:\n",
    "    if db2[y].dtype == object:\n",
    "        db2[y] = db2[y].astype('category')\n",
    "db2['Dog_Fixed'] = db2['Dog_Fixed'].astype('category') \n",
    "db2['DNA_Tested'] = db2['DNA_Tested'].astype('category')\n",
    "db2['Subscribed'] = db2['Subscribed'].astype('category')\n",
    "# categorical data in pandas is like factor data in R\n",
    "\n",
    "# change unreasonable values in the data\n",
    "\n",
    "db2.loc[db2['Weight'] == 0.0,'Weight'] = 0.1\n",
    "# first check the distribution of Weight, and other int and float columns\n",
    "\n",
    "db2.loc[db2['Max_Dogs'] == 0.0, 'Max_Dogs'] = 1.0\n",
    "# first check the distribution of Max_Dogs\n",
    "\n",
    "# convert the column below to ordered category\n",
    "\n",
    "db2['Last_Active_At'] = db2['Last_Active_At'].cat.as_ordered()\n",
    "print db2.dtypes\n",
    "print\n",
    "\n",
    "print 'The columns of ordered category:'\n",
    "for i in db2.columns:\n",
    "    if db2[i].dtype.name == 'category' and db2[i].cat.ordered == True:\n",
    "        print i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: log transformation and feature Scaling for continuous variables\n",
    "\n",
    "For continuous variables, if it is not normal distributuion and has skewed distributions, or if there are many outliers, we will try log (natural log or log10) scale transformation or square root scale transformation. We can try log10(x), sqrt(x), or log10(x+1), sqrt(x+1), after we do this data transform, previous outliers may be proven not at all (we can draw boxplot again with the transformed data). In this case, instead of using x, we will use log10(x) in subsequent modeling process.\n",
    "\n",
    "对于log transformation，数据分布通常是大量数据集中在很小的区域，通过取log，这个很小的区域就能够扩大，看的清楚一些。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.fit_transform(X_test)\n",
    "\n",
    "#-------------------------------------------------------\n",
    "\n",
    "from ggplot import *\n",
    "\n",
    "p = ggplot(db2, aes(x='Total Tests Completed'))\n",
    "# p + geom_histogram()\n",
    "# p + geom_histogram(binwidth=1)\n",
    "p + geom_histogram(bins=20)\n",
    "\n",
    "p = ggplot(db2, aes(x='Gender',y='Total Tests Completed'))\n",
    "# p + geom_violin(alpha = .75)\n",
    "p + geom_violin()\n",
    "\n",
    "import seaborn as sns\n",
    "# seaborn.__version__\n",
    "sns.violinplot(x=db2['Gender'], y=db2['Total Tests Completed'], inner=None, color=\"white\", \n",
    "               cut=0)\n",
    "sns.stripplot(x=db2['Gender'], y=db2['Total Tests Completed'], jitter=.3,  color=\"black\", \n",
    "              alpha=.1, size=4)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "g=sns.boxplot(x=db2['Total Tests Completed'])\n",
    "\n",
    "temp = np.log10(db2['Total Tests Completed'])\n",
    "sns.boxplot(x=temp)\n",
    "\n",
    "sns.boxplot(x=\"Gender\", y=\"Total Tests Completed\", data=db2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Splitting the datasets into training sets and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = 0.2, random_state = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
